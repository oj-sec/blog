<!DOCTYPE html>
<html lang="en" class="dark">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../../favicon.png" />
		<meta name="viewport" content="width=device-width" />
		
		<link href="../../_app/immutable/assets/0.Dunn0vyT.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/ProgressBar.Cirlo5Z8.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/6.Dgtqgz6E.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.BY3Fq3nR.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/entry.Dc7synTk.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/scheduler.DzZHELrM.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.COvA8tIP.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.DED41uK0.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.BWQOah18.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.hrrRJJPh.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/ProgressBar.svelte_svelte_type_style_lang.BrI0fzGN.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/stores.1kU3aqDV.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/4.DRXA-EW0.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CodeBlock.Cigd8BMN.js">
	</head>
	<body data-sveltekit-preload-data="hover" data-theme="wintry">
		<div style="display: contents">    <div class="container h-full mx-auto flex justify-center items-center leading-relaxed"><div class="space-y-5 m-10 custom-container svelte-1bdawuf"><h1 class="h1 text-center mb-12" data-svelte-h="svelte-aomebi">@oj-sec</h1> <h2 class="h2" data-svelte-h="svelte-1ad4dy8">Evaluating Large Language Models as future event forecasters - Part
            Two: Performance &amp; token sampling</h2> <p data-svelte-h="svelte-bomvs2">5 May 2024</p> <p class="card variant-filled-ghost p-4" data-svelte-h="svelte-1g3dbvv">You can access a Juypter notebook (built for Colab) associated with
            this post <a href="https://github.com/oj-sec/blog/blob/main/notebooks/20240505.ipynb" style="text-decoration: underline; color: lightblue;">here</a>.</p> <h3 class="h3" data-svelte-h="svelte-fwvbau">Introduction</h3> <p data-svelte-h="svelte-18k5reb">We <a href="/blog/20240404" style="text-decoration: underline; color: lightblue;">left off</a> making predictions by repeatedly invoking a simple prompt and then
            examining the distribution of results. It&#39;s worth us examining what was
            going on in a little more detail. We got different results as the result
            of our temperature setting, which was configured to 0.7. Temperature
            in AI models is a measure of how stochastic our outputs are. At a temperature
            of zero, a model will always generate the most likely token, resulting
            in deterministic output. As we increase temperature, token selection
            becomes increasingly random, resulting in more diverse outputs. Temperature
            is commonly set in the range of 0.7-1.0 and higher temperature is often
            perceived by users as higher model intelligence and creativity.</p> <p data-svelte-h="svelte-xdxav7">We were using temperature to approximate the model&#39;s tendency to
            choose particular predictions over many samples. But we can achieve
            the same result more directly, by examining the probabilities of
            options straight from the model. This will give us a performance
            speedup equal to the number of samples we intend to take. While
            optimising a system so early is generally a mistake, its hard to
            leave a straightforward 100x or greater performance improvement on
            the table. To understand how we can peer directly into the model&#39;s
            probabilities, we&#39;ll need to understand tokenisation.</p> <h3 class="h3" data-svelte-h="svelte-1oc4nk8">Tokenisation</h3> <p data-svelte-h="svelte-95h6ll">Language models operate on text fragments called tokens, both when
            consuming prompts and generating output. Tokens are semantic
            fragments of written text, typically at the word and sub-word level,
            but sometimes down to the character level. We can tokenise some text
            as an example by directly accessing the tokeniser inside the model
            object maintained for us by Guidance:</p>  <div class="codeblock overflow-hidden shadow bg-neutral-900/90  text-sm text-white rounded-container-token shadow " data-testid="codeblock"> <header class="codeblock-header text-xs text-white/50 uppercase flex justify-between items-center p-2 pl-4"> <span class="codeblock-language">python</span>  <button type="button" class="codeblock-btn btn btn-sm variant-soft !text-white">Copy</button></header>  <pre class="codeblock-pre whitespace-pre-wrap break-all p-4 pt-1"><code class="codeblock-code language-python lineNumbers"><!-- HTML_TAG_START --><span class="hljs-comment"># create a text string to tokenise</span>
string = <span class="hljs-string">&quot;Mindsets tend to be quick to form but resistant to change.&quot;</span>

<span class="hljs-comment"># generate the tokens by accessing the model tokeniser</span>
tokens_encoded = llm.engine.model_obj.tokenize(<span class="hljs-built_in">str</span>.encode(string))

<span class="hljs-comment"># decode the tokens</span>
tokens = []
<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens_encoded:
    <span class="hljs-keyword">if</span> token:
        tokens.append(llm.engine.model_obj.detokenize([token]).decode(<span class="hljs-string">&quot;utf-8&quot;</span>, errors=<span class="hljs-string">&quot;replace&quot;</span>))

<span class="hljs-comment"># show results</span>
<span class="hljs-built_in">print</span>(tokens_encoded)
<span class="hljs-built_in">print</span>(tokens)<!-- HTML_TAG_END --></code></pre></div> <p data-svelte-h="svelte-1c6wsvm">We get back the following output:</p>  <div class="codeblock overflow-hidden shadow bg-neutral-900/90  text-sm text-white rounded-container-token shadow " data-testid="codeblock"> <header class="codeblock-header text-xs text-white/50 uppercase flex justify-between items-center p-2 pl-4"> <span class="codeblock-language">plaintext</span>  <button type="button" class="codeblock-btn btn btn-sm variant-soft !text-white">Copy</button></header>  <pre class="codeblock-pre whitespace-pre-wrap break-all p-4 pt-1"><code class="codeblock-code language-plaintext lineNumbers"><!-- HTML_TAG_START -->[1, 14683, 6591, 6273, 298, 347, 2936, 298, 1221, 562, 605, 11143, 298, 2268, 28723]
[&#x27;&#x27;, &#x27; Mind&#x27;, &#x27;sets&#x27;, &#x27; tend&#x27;, &#x27; to&#x27;, &#x27; be&#x27;, &#x27; quick&#x27;, &#x27; to&#x27;, &#x27; form&#x27;, &#x27; but&#x27;, &#x27; res&#x27;, &#x27;istant&#x27;, &#x27; to&#x27;, &#x27; change&#x27;, &#x27;.&#x27;]<!-- HTML_TAG_END --></code></pre></div> <p data-svelte-h="svelte-j99q6r">We can observe a mixture of whole and partial words in the output.
            We can also observe that words commonly have a leading space, which
            may be important for us to account for in some circumstances. A
            common rule of thumb for evaluating how many tokens are present in a
            particular string is that there are approximately four characters
            per token and approximately one token for every 0.75 words.</p> <h3 class="h3" data-svelte-h="svelte-1q161gx">A note on how Guidance handles tokens</h3> <p data-svelte-h="svelte-x3jlge">When we provide Guidance with a regular expression, possible tokens
            are evaluated against the constraining regular expression and
            discarded if they do not match. This functioning is critical for us
            to understand, because we might otherwise assume that the generation
            is evaluated in larger chunks, like whole words or entire phrases.
            This misunderstanding can result in unexpected behaviour due to the
            model starting to generate a coherent answer that is consistent with
            the start of an incoherent option.</p> <p data-svelte-h="svelte-1pevgl0">We can borrow an example from Guidance&#39;s Github <a href="https://github.com/guidance-ai/guidance/issues/564" style="text-decoration: underline; color: lightblue;">issues</a>:</p>  <div class="codeblock overflow-hidden shadow bg-neutral-900/90  text-sm text-white rounded-container-token shadow " data-testid="codeblock"> <header class="codeblock-header text-xs text-white/50 uppercase flex justify-between items-center p-2 pl-4"> <span class="codeblock-language">python</span>  <button type="button" class="codeblock-btn btn btn-sm variant-soft !text-white">Copy</button></header>  <pre class="codeblock-pre whitespace-pre-wrap break-all p-4 pt-1"><code class="codeblock-code language-python lineNumbers"><!-- HTML_TAG_START --><span class="hljs-keyword">from</span> guidance <span class="hljs-keyword">import</span> models, select

<span class="hljs-comment"># load the model</span>
llm = models.LlamaCpp(<span class="hljs-string">&quot;./models/mistral-7b-openorca.Q4_K_M.gguf&quot;</span>, n_gpu_layers=<span class="hljs-number">20</span>, n_ctx=<span class="hljs-number">4096</span>) 

<span class="hljs-comment"># demonstrate bad generation - note that select() functions identically to a regex of the form &quot;(cloud|skill)&quot;</span>
llm + <span class="hljs-string">&#x27;A word very similar to &quot;sky&quot; is &quot;&#x27;</span> + select([<span class="hljs-string">&quot;cloud&quot;</span>,<span class="hljs-string">&quot;skill&quot;</span>])<!-- HTML_TAG_END --></code></pre></div> <p data-svelte-h="svelte-td0agc">Which gives us the perplexing output <span class="pre p-1">&quot;skill&quot;</span>
            rather than <span class="pre p-1">&quot;cloud&quot;</span>. The model was
            trying to generate a reasonable answer (<span class="pre p-1">&quot;skies&quot;</span>) that collided with the invalid
            <span class="pre p-1">&quot;skill&quot;</span> option. Once the model started
            generating, it could only output
            <span class="pre p-1">&quot;skill&quot;</span> despite the low coherence of the
            answer. As noted by a Guidance contributor, we can address this particular
            case by putting the options directly into the prompt to provide some
            context. We&#39;ll cover another pattern for addressing this issue in the
            next part in this series, but it is critical for us to understand that
            Guidance&#39;s constraints are at the naive token level so that we don&#39;t
            expect contextual evaluation where none is occurring.</p> <h3 class="h3" data-svelte-h="svelte-w3deqj">Sampling token probabilities directly</h3> <p data-svelte-h="svelte-hnqmto">With a background on tokens, we can have a look at directly
            accessing the probability of a particular output. Because generation
            happens token by token, we need to evaluate the probability of each
            token in sequence. These token-specific sequential probabilities
            commonly called the &quot;logprobs&quot; of tokens, defined as log(p) where p
            is the probability of the token occurring given the preceding
            tokens, both generated and prompt. We&#39;re going to stick with
            straight probabilities today, but if you shift this paradigm to a
            different stack, including OpenAI APIs, logprobs is the term to look
            for.</p> <p data-svelte-h="svelte-kkoamv">Unfortunately, Guidance&#39;s interface for accessing logits is fairly
            nascent, so we need to implement the method for accessing
            probabilities ourselves. To keep things from getting too complex for
            now, we can cheat by pregenerating the <span class="pre p-1">&quot;0.&quot;</span> for predictions and instead just evaluating the probabilities of tokens
            in the tenths place. We can access the token logprobs with the following
            code:</p>  <div class="codeblock overflow-hidden shadow bg-neutral-900/90  text-sm text-white rounded-container-token shadow " data-testid="codeblock"> <header class="codeblock-header text-xs text-white/50 uppercase flex justify-between items-center p-2 pl-4"> <span class="codeblock-language">python</span>  <button type="button" class="codeblock-btn btn btn-sm variant-soft !text-white">Copy</button></header>  <pre class="codeblock-pre whitespace-pre-wrap break-all p-4 pt-1"><code class="codeblock-code language-python lineNumbers"><!-- HTML_TAG_START --><span class="hljs-keyword">from</span> guidance <span class="hljs-keyword">import</span> models, gen
<span class="hljs-keyword">import</span> llama_cpp
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> json

<span class="hljs-comment"># load the model</span>
llm = models.LlamaCpp(<span class="hljs-string">&quot;./models/mistral-7b-openorca.Q4_K_M.gguf&quot;</span>, compute_log_probs=<span class="hljs-literal">True</span>, n_gpu_layers=<span class="hljs-number">20</span>, n_ctx=<span class="hljs-number">4096</span>) 

<span class="hljs-comment"># define a regular expression to match a single number</span>
output_regex = <span class="hljs-string">r&quot;\d&quot;</span>

<span class="hljs-comment"># define our prompt - note that we&#x27;ve added a &quot;0.&quot; force output to just examine the tenths place</span>
prompt = <span class="hljs-string">&#x27;Predict the likelihood of the following outcome on a scale from 0.00 to 1.00, with 0.00 meaning the event is impossible and 1.00 meaning the event is certain to occur: &quot;Donald Trump will win the 2024 US election.&quot;\nPREDICTION:0.&#x27;</span>

<span class="hljs-comment"># run constrained inference - note that we have set temperature to zero</span>
output = llm + prompt + gen(name=<span class="hljs-string">&quot;response&quot;</span>, regex=output_regex, max_tokens=<span class="hljs-number">1</span>, temperature=<span class="hljs-number">0.0</span>)

<span class="hljs-comment"># define the options we want to check the probs for</span>
options = [<span class="hljs-string">f&quot;<span class="hljs-subst">{n}</span>&quot;</span> <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>)] 

<span class="hljs-comment"># retrieve the logits from the model object</span>
logits = llama_cpp.llama_get_logits(llm.engine.model_obj.ctx)

<span class="hljs-comment"># tokenize our options</span>
option_tokens = [llm.engine.model_obj.tokenize(<span class="hljs-built_in">str</span>.encode(o)) <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> options]

<span class="hljs-comment"># retrieve just the option token, discarding the &lt;s&gt; added by the tokenizer</span>
option_tokens = [o[<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> option_tokens] 

<span class="hljs-comment"># retrieve the logits for the option</span>
option_logits = [logits[o] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> option_tokens]

<span class="hljs-comment"># convert the logits into probabilities</span>
option_probs = torch.softmax(torch.tensor(option_logits), dim=<span class="hljs-number">0</span>)

<span class="hljs-comment"># typecast to floats </span>
option_probs = [<span class="hljs-built_in">float</span>(o) <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> option_probs]

<span class="hljs-comment"># zip the options and probabilities together</span>
option_probs = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(options, option_probs))

<span class="hljs-comment"># get the top token</span>
top_token = <span class="hljs-built_in">max</span>(option_probs, key=option_probs.get)

<span class="hljs-comment"># print results</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The highest probability option in the tenths place is: <span class="hljs-subst">{top_token.strip()}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The probability distribution for the tenths place is: &quot;</span>)
<span class="hljs-built_in">print</span>(json.dumps(option_probs, indent=<span class="hljs-number">4</span>))<!-- HTML_TAG_END --></code></pre></div> <p data-svelte-h="svelte-1tls2in">Which gives us the following results:</p>  <div class="codeblock overflow-hidden shadow bg-neutral-900/90  text-sm text-white rounded-container-token shadow " data-testid="codeblock"> <header class="codeblock-header text-xs text-white/50 uppercase flex justify-between items-center p-2 pl-4"> <span class="codeblock-language">plaintext</span>  <button type="button" class="codeblock-btn btn btn-sm variant-soft !text-white">Copy</button></header>  <pre class="codeblock-pre whitespace-pre-wrap break-all p-4 pt-1"><code class="codeblock-code language-plaintext lineNumbers"><!-- HTML_TAG_START -->The highest probability option in the tenths place is: 2
The probability distribution for the tenths place is: 
{
    &quot;0&quot;: 0.15142710506916046,
    &quot;1&quot;: 0.15139013528823853,
    &quot;2&quot;: 0.18116815388202667,
    &quot;3&quot;: 0.16766728460788727,
    &quot;4&quot;: 0.12874439358711243,
    &quot;5&quot;: 0.12978236377239227,
    &quot;6&quot;: 0.0467846542596817,
    &quot;7&quot;: 0.02292967587709427,
    &quot;8&quot;: 0.010299457237124443,
    &quot;9&quot;: 0.009806782938539982
}<!-- HTML_TAG_END --></code></pre></div> <p data-svelte-h="svelte-1bc9j38">Here we have shortcut directly to the actual probability
            distribution for the tenths place in our prediction space. In
            effect, this is an instant 100x speedup relative to our previous
            approach of using temperature to iteratively explore this
            distribution by repeated generations. While we&#39;ve kept things simple
            to demonstrate the concept, this approach can be extrapolated to
            multi-token generation by stepping the model through each token.</p> <p data-svelte-h="svelte-1yjvm5r">This is a powerful capability with broad applicability to LLM tasks.
            If we&#39;re working on a jailbreak technique, we can evaluate exactly
            how likely it is to occur to properly assess its risk. If we&#39;re
            working on classification, we can evaluate the confidence of a given
            prediction. If we&#39;re performing finetuning or prompt engineering, we
            get granular insight into whether we&#39;re getting hotter or colder as
            we make changes.</p> <p data-svelte-h="svelte-165w2i3">To finish up, we can repeat our proof of concept showing the actual
            probability distributions for previous proof of concept predictions.
            A little interestingly, the model is less emphatic on both
            predictions than our previous small sample size suggested.</p> <img src="/_app/immutable/assets/predictions.DQZxKzxl.png" alt="Predictions"></div> </div> 
			
			<script>
				{
					__sveltekit_1r9w7ir = {
						base: new URL("../..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("../../_app/immutable/entry/start.BY3Fq3nR.js"),
						import("../../_app/immutable/entry/app.DED41uK0.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 4],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
