{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Research aside - Hallucination detection & LLM explainability\n",
        "\n",
        "NB: May require a T4 GPU and Colab high-RAM to get the model loaded."
      ],
      "metadata": {
        "id": "S4vvEXSQM4D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Setup - Install dependencies"
      ],
      "metadata": {
        "id": "QJdDStMGM-pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets # &> /dev/null"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L4KjvDaChNYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare LLMAgent object\n",
        "\n",
        "We use a single handler class to hold the model and our token attributes to avoid repeating boilerplate."
      ],
      "metadata": {
        "id": "1BvD1MjAQBK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Python script containing explainability and visualisation\n",
        "utilities for large language models.\n",
        "\"\"\"\n",
        "\n",
        "import io\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from datasets import load_dataset\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "class LLMAgent:\n",
        "    \"\"\"\n",
        "    Class encapsulating explainability and visualisation\n",
        "    utilities for large language models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hf_api_key: str = None,\n",
        "        model_id: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        use_gpu: bool = False,\n",
        "        prompt_suffix=\"<|end|>\\n\",\n",
        "        user_prompt_start=\"<|user|>\\n\",\n",
        "        assistant_prompt_start=\"<|assistant|>\\n\",\n",
        "        system_prompt_start=\"<|system|>\\n\",\n",
        "        system_prompt=\"You are a helpful AI assistant.\\n\",\n",
        "        end_token=\"<|end|>\",\n",
        "        eot_token=\"<|endoftext|>\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the LLMAgent object.\n",
        "        \"\"\"\n",
        "        logging.info(\n",
        "            \"LLMAgent downloading/ensuring presence of large language model: %s.\",\n",
        "            model_id,\n",
        "        )\n",
        "        self.device = \"cuda\" if use_gpu else \"mps\" if torch.has_mps else \"cpu\"\n",
        "        logging.info(\"LLMAgent sending model to device: %s\", self.device)\n",
        "        self.processor = AutoProcessor.from_pretrained(model_id, token=hf_api_key)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            # torch_dtype=torch.bfloat16,\n",
        "            token=hf_api_key,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=True,\n",
        "            do_sample=True,\n",
        "        ).to(self.device)\n",
        "        self.prompt_suffix = prompt_suffix\n",
        "        self.user_prompt_start = user_prompt_start\n",
        "        self.assistant_prompt_start = assistant_prompt_start\n",
        "        self.end_token = end_token\n",
        "        self.eot_token = eot_token\n",
        "        self.system_prompt_start = system_prompt_start\n",
        "        self.system_prompt = system_prompt\n",
        "        logging.info(\"LLMAgent initialisaed.\")\n",
        "\n",
        "    def generate_with_response_dict(\n",
        "        self, prompt: str, max_tokens: int = 200, temperature=0.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Method to inference on the model and return the full response dict,\n",
        "        containing attentions, hidden states, and the generated response.\n",
        "\n",
        "        Args:\n",
        "            prompt: str: The prompt to generate a response for.\n",
        "            max_tokens: int: The maximum number of tokens to generate.\n",
        "\n",
        "        Returns:\n",
        "            dict: The full response dict containing attentions, hidden states, and the generated response.\n",
        "        \"\"\"\n",
        "        logging.info(\"LLMAgent generating response for: %s.\", prompt)\n",
        "        formatted_prompt = f\"{self.system_prompt_start}{self.system_prompt}{self.prompt_suffix}{self.user_prompt_start}{prompt}{self.prompt_suffix}{self.assistant_prompt_start}\"\n",
        "        inputs = self.processor(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            return_dict_in_generate=True,\n",
        "            do_sample=bool(temperature > 0.0),\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        logging.info(\"LLMAgent response generated.\")\n",
        "        return outputs\n",
        "\n",
        "    def visualise_average_activations(self, outputs):\n",
        "        \"\"\"\n",
        "        Method to visualise average activations per layer as a heatmap.\n",
        "        \"\"\"\n",
        "        logging.info(\"LLMAgent visualising average activations for sequence.\")\n",
        "        tokens = [\n",
        "            self.processor.decode(input_token) for input_token in outputs.sequences[0]\n",
        "        ]\n",
        "        average_activations = []\n",
        "        for layer_states in outputs.hidden_states[0]:\n",
        "            avg_activation = layer_states.squeeze(0).mean(dim=-1)\n",
        "            average_activations.append(avg_activation)\n",
        "\n",
        "        for layer_states in outputs.hidden_states[1:]:\n",
        "            for i, layer_state in enumerate(layer_states):\n",
        "                avg_activation = layer_state.squeeze(0).mean(dim=-1)\n",
        "                average_activations[i] = torch.cat(\n",
        "                    [average_activations[i], avg_activation]\n",
        "                )\n",
        "\n",
        "        average_activations = torch.stack(average_activations, dim=1)\n",
        "        figsize_x = max(12, len(outputs.hidden_states[0]) * 0.8)\n",
        "        figsize_y = max(8, len(tokens) * 0.3)\n",
        "\n",
        "        plt.figure(figsize=(figsize_x, figsize_y))\n",
        "        sns.heatmap(\n",
        "            average_activations.detach().cpu().numpy(),\n",
        "            cmap=\"mako_r\",\n",
        "            xticklabels=[f\"Layer {i}\" for i in range(len(outputs.hidden_states[0]))],\n",
        "            yticklabels=tokens,\n",
        "            linecolor=\"lightgrey\",\n",
        "            linewidths=0.2,\n",
        "            cbar=True,\n",
        "        )\n",
        "        plt.title(\"Average activation per layer per token\")\n",
        "        plt.tight_layout()\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format=\"png\")\n",
        "        buffer.seek(0)\n",
        "        image_bytes = buffer.read()\n",
        "        plt.close()\n",
        "        logging.info(\"LLMAgent visualised average activations for sequence.\")\n",
        "        return image_bytes\n",
        "\n",
        "    def generate_with_probability(\n",
        "        self,\n",
        "        prompt,\n",
        "        response_prefix=None,\n",
        "        max_tokens=200,\n",
        "        temperature=0.0,\n",
        "        round_to=6,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Method to generate a response with information about\n",
        "        token probabilities.\n",
        "\n",
        "        Args:\n",
        "            prompt: str: The prompt to generate a response for.\n",
        "            response_prefix: str: String to prefix the LLM's response.\n",
        "            max_tokens: int: The maximum number of tokens to generate.\n",
        "            temperature: float: The temperature to use for sampling.\n",
        "            round_to: int: The number of decimal places to round to.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated response.\n",
        "            float: The total probability of the generated response.\n",
        "            float: The average probability of each token in the generated response.\n",
        "            list[tuple]: The individual token probabilities.\n",
        "        \"\"\"\n",
        "        formatted_prompt = f\"{self.system_prompt_start}{self.system_prompt}{self.prompt_suffix}{self.user_prompt_start}{prompt}{self.prompt_suffix}{self.assistant_prompt_start}\"\n",
        "        if response_prefix:\n",
        "            formatted_prompt = formatted_prompt + response_prefix\n",
        "        logging.info(\"LLMAgent generating response with probability for: %s.\", prompt)\n",
        "        inputs = self.processor(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
        "        generate_output = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True,\n",
        "            do_sample=bool(temperature > 0.0),\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        generate_ids = generate_output.sequences[:, inputs[\"input_ids\"].shape[1] :]\n",
        "        generated_sequence = generate_ids[0].cpu().numpy()\n",
        "        generated_sequence = generated_sequence[:-2]  # Remove eos tokens\n",
        "        response = self.processor.batch_decode(\n",
        "            [generated_sequence],\n",
        "            skip_special_tokens=False,\n",
        "            clean_up_tokenization_spaces=False,\n",
        "        )[0]\n",
        "        logging.info(\"  LLMAgent generated response: '%s'\", response)\n",
        "        logits = torch.stack(generate_output.scores, dim=1).cpu()\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1).cpu()\n",
        "        log_likelihood_for_gen = sum(\n",
        "            log_probs[0, i, token_id].item()\n",
        "            for i, token_id in enumerate(generated_sequence)\n",
        "        )\n",
        "        total_probability_for_gen = round(np.exp(log_likelihood_for_gen), round_to)\n",
        "        individual_token_probs = []\n",
        "        for i, token_id in enumerate(generated_sequence):\n",
        "            token_prob = np.exp(log_probs[0, i, token_id].item())\n",
        "            individual_token_probs.append(\n",
        "                (self.processor.decode(token_id), round(token_prob, round_to))\n",
        "            )\n",
        "        average_token_propabiility = round(\n",
        "            sum(token_prob for _, token_prob in individual_token_probs)\n",
        "            / len(individual_token_probs),\n",
        "            round_to,\n",
        "        )\n",
        "        return (\n",
        "            response,\n",
        "            total_probability_for_gen,\n",
        "            average_token_propabiility,\n",
        "            individual_token_probs,\n",
        "        )\n",
        "\n",
        "    def test_on_triviaQA(self, n=100):\n",
        "        \"\"\"\n",
        "        Method to test hallucination detection methods on the\n",
        "        TriviaQA dataset.\n",
        "\n",
        "        Args:\n",
        "            n: int: The number of samples to test on.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The results of the test.\n",
        "        \"\"\"\n",
        "        logging.info(\n",
        "            \"LLMAgent testing hallucination detection on TriviaQA dataset with %s samples.\",\n",
        "            n,\n",
        "        )\n",
        "        column_headers = [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"response\",\n",
        "            \"total_probability\",\n",
        "            \"average_token_probability\",\n",
        "            \"individual_token_probs\",\n",
        "        ]\n",
        "        df = pd.DataFrame(columns=column_headers)\n",
        "        dataset = load_dataset(\"trivia_qa\", \"rc\", split=\"train\", streaming=True)\n",
        "        iterator = iter(dataset)\n",
        "        for i in range(n):\n",
        "            logging.info(\"LLMAgent processing TriviaQA sample %s of %s.\", i + 1, n)\n",
        "            entry = next(iterator)\n",
        "            question = entry[\"question\"]\n",
        "            answer = entry[\"answer\"][\"value\"]\n",
        "            response, total_probability, average_token_probability, _ = (\n",
        "                self.generate_with_probability(question)\n",
        "            )\n",
        "            row = {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"response\": response,\n",
        "                \"total_probability\": total_probability,\n",
        "                \"average_token_probability\": average_token_probability,\n",
        "            }\n",
        "            row_df = pd.DataFrame([row], columns=column_headers)\n",
        "            df = pd.concat([df, row_df], ignore_index=True)\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "77DegPNvQA0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate object and load model"
      ],
      "metadata": {
        "id": "cYNxX-wWiHiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = LLMAgent(\n",
        "    system_prompt=\"You are a helpful AI assistant that provides correct information as concisely as possible.\\n\",\n",
        "    use_gpu=True\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d8iKgHQbh2kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show generation with propabilities\n",
        "\n",
        "Generating a completion from a Transformers backend with full logit information."
      ],
      "metadata": {
        "id": "0MxP4BTFfUEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "response, total_probability, average_token_probability, individual_token_probs = (\n",
        "    lm.generate_with_probability(\"What is the capital of Australia?\")\n",
        ")\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(f\"Response generated:\\n{response}\")\n",
        "print(f\"Total response probability:\\n{total_probability}\")\n",
        "print(f\"Average token probability:\\n{average_token_probability}\")\n",
        "print(\"Individual token probabilities:\")\n",
        "for token in individual_token_probs:\n",
        "    buff = 20 * \" \"\n",
        "    buff = token[0] + buff[len(token[0]) :]\n",
        "    print(f\"{buff} : {token[1]}\")"
      ],
      "metadata": {
        "id": "oXkz-Bx2ffVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run tests on the TriviaQA dataset"
      ],
      "metadata": {
        "id": "NFFa25nSiWET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = lm.test_on_triviaQA()\n",
        "results.to_csv(\"triviaQA_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "sHh9EvNKicGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate hidden layer activation heatmaps"
      ],
      "metadata": {
        "id": "bfnUuqbDjDjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "generated_response = lm.generate_with_response_dict(\"What is the capital of Australia?\")\n",
        "image_bytes = lm.visualise_average_activations(generated_response)\n",
        "\n",
        "with open(\"./average_activations.png\", \"wb\") as f:\n",
        "    f.write(image_bytes)\n",
        "\n",
        "display(Image(\"./average_activations.png\"))"
      ],
      "metadata": {
        "id": "KTyM2ASJjHxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}