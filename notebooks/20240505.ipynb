{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70fa283ac6f747c1af8d59bd848345b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c36c7cd101904f26a134dad22f428968",
              "IPY_MODEL_a4d4fc41b51544a09c0ea5d8a94ca190",
              "IPY_MODEL_392afffc7a9c49dd9f19c4144c19f062"
            ],
            "layout": "IPY_MODEL_f634ea6596554bb5a5c5ccfe871c1f8e"
          }
        },
        "c36c7cd101904f26a134dad22f428968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7445f45774d4b4f9510a4eb16f298fe",
            "placeholder": "​",
            "style": "IPY_MODEL_5a677aaade82471c8deb4fdd996602cf",
            "value": "Fetching 1 files: 100%"
          }
        },
        "a4d4fc41b51544a09c0ea5d8a94ca190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f666e9721e84db9b20ddd310b8309d3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ea231624d6e40d9a6065f7acaee8a8d",
            "value": 1
          }
        },
        "392afffc7a9c49dd9f19c4144c19f062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c1a57ac37cc4758b977e25bcd5a9ce8",
            "placeholder": "​",
            "style": "IPY_MODEL_ac3e2309f7c7427ea59f04ce7bae134b",
            "value": " 1/1 [00:00&lt;00:00,  5.72it/s]"
          }
        },
        "f634ea6596554bb5a5c5ccfe871c1f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7445f45774d4b4f9510a4eb16f298fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a677aaade82471c8deb4fdd996602cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f666e9721e84db9b20ddd310b8309d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea231624d6e40d9a6065f7acaee8a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c1a57ac37cc4758b977e25bcd5a9ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac3e2309f7c7427ea59f04ce7bae134b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Large Language Models as future event forecasters - Part Two"
      ],
      "metadata": {
        "id": "S4vvEXSQM4D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup - Install dependencies and download model\n",
        "\n",
        "Note that we provide a compilation argument when installing llama-cpp-python to compile llama.cpp with GPU support. This is a very important step to getting tolerable generation speeds, so [read up](https://github.com/ggerganov/llama.cpp#Build) on installing with the right acceleration for your hardware if reusing this code outside of Collab."
      ],
      "metadata": {
        "id": "QJdDStMGM-pi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u0TRUOB6Lb4w"
      },
      "outputs": [],
      "source": [
        "# This will take a while\n",
        "!pip install guidance &> /dev/null\n",
        "!pip install huggingface-hub &> /dev/null\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.27 &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"TheBloke/Mistral-7B-OpenOrca-GGUF\", local_dir=\"models\", allow_patterns=[\"mistral-7b-openorca.Q4_K_M.gguf\"])"
      ],
      "metadata": {
        "id": "4Lf_LKRsPY-r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "70fa283ac6f747c1af8d59bd848345b6",
            "c36c7cd101904f26a134dad22f428968",
            "a4d4fc41b51544a09c0ea5d8a94ca190",
            "392afffc7a9c49dd9f19c4144c19f062",
            "f634ea6596554bb5a5c5ccfe871c1f8e",
            "b7445f45774d4b4f9510a4eb16f298fe",
            "5a677aaade82471c8deb4fdd996602cf",
            "6f666e9721e84db9b20ddd310b8309d3",
            "7ea231624d6e40d9a6065f7acaee8a8d",
            "7c1a57ac37cc4758b977e25bcd5a9ce8",
            "ac3e2309f7c7427ea59f04ce7bae134b"
          ]
        },
        "outputId": "33720a86-213d-49c7-eff1-572b5afd5d13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70fa283ac6f747c1af8d59bd848345b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenising text\n",
        "\n",
        "Below is an example of accessing a llama.cpp model's tokeniser when using Guidance.\n"
      ],
      "metadata": {
        "id": "1BvD1MjAQBK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the modules we want to use\n",
        "from guidance import models, gen\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# load our model into memory\n",
        "llm = models.LlamaCpp(\"./models/mistral-7b-openorca.Q4_K_M.gguf\", n_gpu_layers=20, n_ctx=1000)\n",
        "\n",
        "# create a text string to tokenise\n",
        "string = \"Mindsets tend to be quick to form but resistant to change.\"\n",
        "\n",
        "# generate the tokens by accessing the model tokeniser\n",
        "tokens_encoded = llm.engine.model_obj.tokenize(str.encode(string))\n",
        "\n",
        "# decode the tokens\n",
        "tokens = []\n",
        "for token in tokens_encoded:\n",
        "    if token:\n",
        "        tokens.append(llm.engine.model_obj.detokenize([token]).decode(\"utf-8\", errors=\"replace\"))\n",
        "\n",
        "# clear the output so we can see results\n",
        "clear_output(wait=True)\n",
        "\n",
        "# show results\n",
        "print(tokens_encoded)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "77DegPNvQA0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6481bfe-f012-4e49-953b-0948d0cd732f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14683, 6591, 6273, 298, 347, 2936, 298, 1221, 562, 605, 11143, 298, 2268, 28723]\n",
            "['', ' Mind', 'sets', ' tend', ' to', ' be', ' quick', ' to', ' form', ' but', ' res', 'istant', ' to', ' change', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aside: How Guidance handles token constraints\n",
        "\n",
        "This example demonstrates how Guidance's handling of token-level constraints can result in unexpected behaviour. Example taken from: https://github.com/guidance-ai/guidance/issues/564"
      ],
      "metadata": {
        "id": "4jdyjsseXr_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from guidance import models, select\n",
        "\n",
        "# load the model\n",
        "llm = models.LlamaCpp(\"./models/mistral-7b-openorca.Q4_K_M.gguf\", n_gpu_layers=20, n_ctx=4096)\n",
        "\n",
        "# deliberately trigger an incoherent generation\n",
        "llm + 'A word very similar to \"sky\" is \"' + select([\"cloud\",\"skill\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "id": "MlfwEioZX7Tc",
        "outputId": "aa06dfcb-d56b-4aa1-8db4-e1aa202889e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<guidance.models.llama_cpp._llama_cpp.LlamaCpp at 0x7fbb9990b2b0>"
            ],
            "text/html": [
              "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>A word very similar to &quot;sky&quot; is &quot;<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>s</span>kill</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust the prompt to introduce a coherent generation\n",
        "llm + \"Which word (cloud or skill) is more similar to sky? The more similar is \" + select([\"cloud\",\"skill\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "id": "pVjnVqzhW28Q",
        "outputId": "b0eacbc4-13cd-4bde-eb6d-18c9a849abd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<guidance.models.llama_cpp._llama_cpp.LlamaCpp at 0x7fbb9990b550>"
            ],
            "text/html": [
              "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Which word (cloud or skill) is more similar to sky? The more similar is <span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>cloud</span></pre>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing logprobs\n",
        "\n",
        "We can gain a huge peformance increase for our use case by accessing the propability that each candidate will be generated. We can access the logpobs by examining tokens in the model's cache following a generation. Note that this is a simple implementation of a concept you can use to chain through a multi-token candidate."
      ],
      "metadata": {
        "id": "EngHCghYUfIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from guidance import models, gen\n",
        "import llama_cpp\n",
        "import torch\n",
        "import math\n",
        "import json\n",
        "\n",
        "# load the model\n",
        "llm = models.LlamaCpp(\"./models/mistral-7b-openorca.Q4_K_M.gguf\", compute_log_probs=True, n_gpu_layers=20, n_ctx=4096)\n",
        "\n",
        "# define a regular expression to match a single number\n",
        "output_regex = r\"\\d\"\n",
        "\n",
        "# define our prompt - note that we've added a \"0.\" force output to just examine the tenths place\n",
        "prompt = 'Predict the likelihood of the following outcome on a scale from 0.00 to 1.00, with 0.00 meaning the event is impossible and 1.00 meaning the event is certain to occur: \"Donald Trump will win the 2024 US election.\"\\nPREDICTION:0.'\n",
        "\n",
        "# run constrained inference - noting that we have set temperature to zero\n",
        "output = llm + prompt + gen(name=\"response\", regex=output_regex, max_tokens=1, temperature=0.0)\n",
        "\n",
        "# define the options we want to check the probs for\n",
        "options = [f\"{n}\" for n in range(0,10)]\n",
        "\n",
        "# retrieve the logits from the model object\n",
        "logits = llama_cpp.llama_get_logits(llm.engine.model_obj.ctx)\n",
        "\n",
        "# tokenize our options\n",
        "option_tokens = [llm.engine.model_obj.tokenize(str.encode(o)) for o in options]\n",
        "\n",
        "# retrieve just the option token, discarding the <s> added by the tokenizer\n",
        "option_tokens = [o[2] for o in option_tokens]\n",
        "\n",
        "# retrieve the logits for the option\n",
        "option_logits = [logits[o] for o in option_tokens]\n",
        "\n",
        "# convert the logits into propabilities\n",
        "option_probs = torch.softmax(torch.tensor(option_logits), dim=0)\n",
        "\n",
        "# cast the softmaxes to floats\n",
        "option_probs = [(float(o)) for o in option_probs]\n",
        "\n",
        "# we could alternatively deal with logprobs directly here\n",
        "# option_probs = [math.log(float(o)) for o in option_probs]\n",
        "\n",
        "# zip the options and logprobs together\n",
        "option_probs = dict(zip(options, option_probs))\n",
        "\n",
        "# get the top token\n",
        "top_token = max(option_probs, key=option_probs.get)\n",
        "\n",
        "# print results\n",
        "print(f\"The highest probability option in the tenths place is: {top_token.strip()}\")\n",
        "print(\"The probability distribution for the tenths place is: \")\n",
        "print(json.dumps(option_probs, indent=4))\n",
        "print(sum(option_probs.values()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "kvFtYxfMXNWX",
        "outputId": "c6aa1737-1cac-4983-fa9b-5ff1b5053e4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Predict the likelihood of the following outcome on a scale from 0.00 to 1.00, with 0.00 meaning the event is impossible and 1.00 meaning the event is certain to occur: &quot;Donald Trump will win the 2024 US election.&quot;\n",
              "PREDICTION:0.<span style='background-color: rgba(135.11468481899684, 29.885315181003165, 0, 0.15); border-radius: 3px;' title='0.1811231223091101'>2</span></pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The highest probability option in the tenths place is: 2\n",
            "The probability distribution for the tenths place is: \n",
            "{\n",
            "    \"0\": 0.15142710506916046,\n",
            "    \"1\": 0.15139013528823853,\n",
            "    \"2\": 0.18116815388202667,\n",
            "    \"3\": 0.16766728460788727,\n",
            "    \"4\": 0.12874439358711243,\n",
            "    \"5\": 0.12978236377239227,\n",
            "    \"6\": 0.0467846542596817,\n",
            "    \"7\": 0.02292967587709427,\n",
            "    \"8\": 0.010299457237124443,\n",
            "    \"9\": 0.009806782938539982\n",
            "}\n",
            "1.000000006519258\n"
          ]
        }
      ]
    }
  ]
}