{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Large Language Models as future event forecasters - Part One"
      ],
      "metadata": {
        "id": "S4vvEXSQM4D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup - Install dependencies and download model\n",
        "\n",
        "Note that we provide a compilation argument when installing llama-cpp-python to compile llama.cpp with GPU support. This is a very important step to getting tolerable generation speeds, so [read up](https://github.com/ggerganov/llama.cpp#Build) on installing with the right acceleration for your hardware if reusing this code outside of Collab."
      ],
      "metadata": {
        "id": "QJdDStMGM-pi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0TRUOB6Lb4w"
      },
      "outputs": [],
      "source": [
        "# This will take a while\n",
        "!pip install guidance &> /dev/null\n",
        "!pip install huggingface-hub &> /dev/null\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.27 &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"TheBloke/Mistral-7B-OpenOrca-GGUF\", local_dir=\"models\", allow_patterns=[\"mistral-7b-openorca.Q4_K_M.gguf\"])"
      ],
      "metadata": {
        "id": "4Lf_LKRsPY-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constrained generation\n",
        "\n",
        "Below is an example of using Guidance to constrain the output from a language model to a particular regular expression.\n"
      ],
      "metadata": {
        "id": "1BvD1MjAQBK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the modules we want to use\n",
        "from guidance import models, gen\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# load our model into memory\n",
        "llm = models.LlamaCpp(\"./models/mistral-7b-openorca.Q4_K_M.gguf\", n_gpu_layers=20, n_ctx=1000)\n",
        "\n",
        "# create our prompt and forecast\n",
        "prompt = 'Predict the likelihood of the following outcome on a scale from 0 to 1, with 0 meaning the event is impossible and 1 meaning the event is certain to occur: \"Donald Trump will win the 2024 US election.\"\\nPREDICTION:'\n",
        "\n",
        "# use the model to generate with no constraint\n",
        "output = llm + prompt + gen(name=\"response\", max_tokens=100, temperature=0.7)\n",
        "unconstrained_response = output['response']\n",
        "\n",
        "# constrain the model to generate the format we want\n",
        "output_regex = r\"(0\\.\\d\\d|1\\.00)\"\n",
        "output = llm + prompt + gen(name=\"response\", regex=output_regex, max_tokens=10, temperature=0.7)\n",
        "constrained_response = output['response']\n",
        "\n",
        "# clear the output so we can see results\n",
        "clear_output(wait=True)\n",
        "\n",
        "# show our results\n",
        "print(f\"Unconstrained output was:\\n{unconstrained_response}\\n\\nConstrained output was:\\n{constrained_response}\")\n"
      ],
      "metadata": {
        "id": "77DegPNvQA0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proof of concept\n",
        "\n",
        "Below we run the forecast many times for a contentious and non-contentious prompts. We then visualise histograms to get a feel for the distribution of forecasts a low power model can make with no additional contextual information.\n",
        "\n",
        "For some forecasts, we will see a neat normal distribution, which indicates there is some internally consistent \"reasoning\" occurring. We'll revisit this idea of \"sampling\" a model that we are deliberately making random using temperature later."
      ],
      "metadata": {
        "id": "ZVdNd6Plj03I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_trump = []\n",
        "predictions_horse = []"
      ],
      "metadata": {
        "id": "obKwKJuGoXvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_trump = \"Donald Trump will win the 2024 US election.\"\n",
        "forecast_horse = \"A horse will win the 2024 US election.\"\n",
        "\n",
        "for i in range(100):\n",
        "    output_regex = r\"(0\\.\\d\\d|1\\.00)\"\n",
        "    output = llm + f'Predict the likelihood of the following outcome on a scale from 0.00 to 1.00, with 0.00 meaning the event is impossible and 1.00 meaning the event is certain to occur: \"{forecast_trump}\"\\nPREDICTION:' + gen(name=\"response\", regex=output_regex, max_tokens=10, temperature=0.7)\n",
        "    predictions_trump.append(output['response'])\n",
        "    output = llm + f'Predict the likelihood of the following outcome on a scale from 0.00 to 1.00, with 0.00 meaning the event is impossible and 1.00 meaning the event is certain to occur: \"{forecast_horse}\"\\nPREDICTION:' + gen(name=\"response\", regex=output_regex, max_tokens=10, temperature=0.7)\n",
        "    predictions_horse.append(output['response'])\n"
      ],
      "metadata": {
        "id": "M-i1uDeafivn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data1 = [(float(prediction) + 1e-10) for prediction in predictions_trump]\n",
        "data2 = [(float(prediction) + 1e-10) for prediction in predictions_horse]\n",
        "\n",
        "bins = np.arange(0, 1.1, 0.1)\n",
        "hist1, _ = np.histogram(data1, bins=bins)\n",
        "hist2, _ = np.histogram(data2, bins=bins)\n",
        "max_count = max(max(hist1), max(hist2))\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(np.arange(len(hist1)), hist1, align='center', alpha=0.7)\n",
        "plt.xlabel('Prediction range')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Trump 2024')\n",
        "plt.xticks(np.arange(len(hist1)), ['{:.1f}-{:.1f}'.format(bins[i], bins[i+1]) for i in range(len(hist1))], rotation=45, ha='right')\n",
        "plt.ylim(0, max_count)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(np.arange(len(hist2)), hist2, align='center', alpha=0.7)\n",
        "plt.xlabel('Prediction range')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Horse 2024')\n",
        "plt.xticks(np.arange(len(hist2)), ['{:.1f}-{:.1f}'.format(bins[i], bins[i+1]) for i in range(len(hist2))], rotation=45, ha='right')\n",
        "plt.ylim(0, max_count)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N0sZaDnxgLdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}