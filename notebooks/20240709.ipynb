{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4vvEXSQM4D0"
      },
      "source": [
        "# Research aside - Hallucination detection & LLM explainability - Part 2\n",
        "\n",
        "NB: May require a T4 GPU and Colab high-RAM to get the model loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJdDStMGM-pi"
      },
      "source": [
        "\n",
        "## Setup - Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "L4KjvDaChNYp"
      },
      "outputs": [],
      "source": [
        "!pip install datasets # &> /dev/null\n",
        "!pip install sentence-transformers # &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BvD1MjAQBK_"
      },
      "source": [
        "## Declare LLMAgent object\n",
        "\n",
        "We use a single handler class to hold the model and our token attributes to avoid repeating boilerplate.\n",
        "\n",
        "This object has some additional functionality inside \"test_on_triviaQA()\" to automatically evaluate answers based on cosine similarity, supported by a attribute on LLMAgent containing an embedding model.\n",
        "\n",
        "LLM used: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
        "\n",
        "Embedding model used: https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77DegPNvQA0m"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Python script containing explainability and visualisation\n",
        "utilities for large language models.\n",
        "\"\"\"\n",
        "\n",
        "import io\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "class LLMAgent:\n",
        "    \"\"\"\n",
        "    Class encapsulating explainability and visualisation\n",
        "    utilities for large language models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hf_api_key: str = None,\n",
        "        embedding_model_id: str = \"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "        embedding_instruction: str = \"Represent this sentence for searching relevant passages:\",\n",
        "        embedding_sameness_threshold: float = 0.67,\n",
        "        model_id: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        use_gpu: bool = False,\n",
        "        prompt_suffix=\"<|end|>\\n\",\n",
        "        user_prompt_start=\"<|user|>\\n\",\n",
        "        assistant_prompt_start=\"<|assistant|>\\n\",\n",
        "        system_prompt_start=\"<|system|>\\n\",\n",
        "        system_prompt=\"You are a helpful AI assistant that provides concise answers.\\n\",\n",
        "        end_token=\"<|end|>\",\n",
        "        eot_token=\"<|endoftext|>\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the LLMAgent object.\n",
        "        \"\"\"\n",
        "        logging.info(\"LLMAgent initialising.\")\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_id, device=\"cpu\")\n",
        "        self.embedding_instruction = embedding_instruction\n",
        "        self.embedding_sameness_threshold = embedding_sameness_threshold\n",
        "        logging.info(\n",
        "            \"LLMAgent downloading/ensuring presence of large language model: %s.\",\n",
        "            model_id,\n",
        "        )\n",
        "        self.device = \"cuda\" if use_gpu else \"mps\" if torch.has_mps else \"cpu\"\n",
        "        logging.info(\"LLMAgent sending model to device: %s\", self.device)\n",
        "        self.processor = AutoProcessor.from_pretrained(model_id, token=hf_api_key)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            # torch_dtype=torch.bfloat16,\n",
        "            token=hf_api_key,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=True,\n",
        "            do_sample=True,\n",
        "        ).to(self.device)\n",
        "        self.prompt_suffix = prompt_suffix\n",
        "        self.user_prompt_start = user_prompt_start\n",
        "        self.assistant_prompt_start = assistant_prompt_start\n",
        "        self.end_token = end_token\n",
        "        self.eot_token = eot_token\n",
        "        self.system_prompt_start = system_prompt_start\n",
        "        self.system_prompt = system_prompt\n",
        "        logging.info(\"LLMAgent initialisaed.\")\n",
        "\n",
        "    def generate_with_response_dict(\n",
        "        self, prompt: str, max_tokens: int = 200, temperature=0.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Method to inference on the model and return the full response dict,\n",
        "        containing attentions, hidden states, and the generated response.\n",
        "\n",
        "        Args:\n",
        "            prompt: str: The prompt to generate a response for.\n",
        "            max_tokens: int: The maximum number of tokens to generate.\n",
        "\n",
        "        Returns:\n",
        "            dict: The full response dict containing attentions, hidden states, and the generated response.\n",
        "        \"\"\"\n",
        "        logging.info(\"LLMAgent generating response for: %s.\", prompt)\n",
        "        formatted_prompt = f\"{self.system_prompt_start}{self.system_prompt}{self.prompt_suffix}{self.user_prompt_start}{prompt}{self.prompt_suffix}{self.assistant_prompt_start}\"\n",
        "        inputs = self.processor(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            return_dict_in_generate=True,\n",
        "            do_sample=bool(temperature > 0.0),\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        logging.info(\"LLMAgent response generated.\")\n",
        "        return outputs\n",
        "\n",
        "    def visualise_average_activations(self, outputs):\n",
        "        \"\"\"\n",
        "        Method to visualise average activations per layer as a heatmap.\n",
        "        \"\"\"\n",
        "        logging.info(\"LLMAgent visualising average activations for sequence.\")\n",
        "        tokens = [\n",
        "            self.processor.decode(input_token) for input_token in outputs.sequences[0]\n",
        "        ]\n",
        "        average_activations = []\n",
        "        for layer_states in outputs.hidden_states[0]:\n",
        "            avg_activation = layer_states.squeeze(0).mean(dim=-1)\n",
        "            average_activations.append(avg_activation)\n",
        "\n",
        "        for layer_states in outputs.hidden_states[1:]:\n",
        "            for i, layer_state in enumerate(layer_states):\n",
        "                avg_activation = layer_state.squeeze(0).mean(dim=-1)\n",
        "                average_activations[i] = torch.cat(\n",
        "                    [average_activations[i], avg_activation]\n",
        "                )\n",
        "\n",
        "        average_activations = torch.stack(average_activations, dim=1)\n",
        "        figsize_x = max(12, len(outputs.hidden_states[0]) * 0.8)\n",
        "        figsize_y = max(8, len(tokens) * 0.3)\n",
        "\n",
        "        plt.figure(figsize=(figsize_x, figsize_y))\n",
        "        sns.heatmap(\n",
        "            average_activations.detach().cpu().numpy(),\n",
        "            cmap=\"mako_r\",\n",
        "            xticklabels=[f\"Layer {i}\" for i in range(len(outputs.hidden_states[0]))],\n",
        "            yticklabels=tokens,\n",
        "            linecolor=\"lightgrey\",\n",
        "            linewidths=0.2,\n",
        "            cbar=True,\n",
        "        )\n",
        "        plt.title(\"Average activation per layer per token\")\n",
        "        plt.tight_layout()\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format=\"png\")\n",
        "        buffer.seek(0)\n",
        "        image_bytes = buffer.read()\n",
        "        plt.close()\n",
        "        logging.info(\"LLMAgent visualised average activations for sequence.\")\n",
        "        return image_bytes\n",
        "\n",
        "    def generate_with_probability(\n",
        "        self,\n",
        "        prompt,\n",
        "        response_prefix=None,\n",
        "        max_tokens=200,\n",
        "        temperature=0.0,\n",
        "        round_to=6,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Method to generate a response with information about\n",
        "        token probabilities.\n",
        "\n",
        "        Args:\n",
        "            prompt: str: The prompt to generate a response for.\n",
        "            response_prefix: str: String to prefix the LLM's response.\n",
        "            max_tokens: int: The maximum number of tokens to generate.\n",
        "            temperature: float: The temperature to use for sampling.\n",
        "            round_to: int: The number of decimal places to round to.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated response.\n",
        "            float: The total probability of the generated response.\n",
        "            float: The average probability of each token in the generated response.\n",
        "            list[tuple]: The individual token probabilities.\n",
        "            dict: The full generation output.\n",
        "        \"\"\"\n",
        "        formatted_prompt = f\"{self.system_prompt_start}{self.system_prompt}{self.prompt_suffix}{self.user_prompt_start}{prompt}{self.prompt_suffix}{self.assistant_prompt_start}\"\n",
        "        if response_prefix:\n",
        "            formatted_prompt = formatted_prompt + response_prefix\n",
        "        logging.info(\"LLMAgent generating response with probability for: %s.\", prompt)\n",
        "        inputs = self.processor(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
        "        generate_output = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True,\n",
        "            do_sample=bool(temperature > 0.0),\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        generate_ids = generate_output.sequences[:, inputs[\"input_ids\"].shape[1] :]\n",
        "        generated_sequence = generate_ids[0].cpu().numpy()\n",
        "        generated_sequence = generated_sequence[:-2]  # Remove eos tokens\n",
        "        response = self.processor.batch_decode(\n",
        "            [generated_sequence],\n",
        "            skip_special_tokens=False,\n",
        "            clean_up_tokenization_spaces=False,\n",
        "        )[0]\n",
        "        logging.info(\"  LLMAgent generated response: '%s'\", response)\n",
        "        logits = torch.stack(generate_output.scores, dim=1).cpu()\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1).cpu()\n",
        "        log_likelihood_for_gen = sum(\n",
        "            log_probs[0, i, token_id].item()\n",
        "            for i, token_id in enumerate(generated_sequence)\n",
        "        )\n",
        "        total_probability_for_gen = round(np.exp(log_likelihood_for_gen), round_to)\n",
        "        individual_token_probs = []\n",
        "        for i, token_id in enumerate(generated_sequence):\n",
        "            token_prob = np.exp(log_probs[0, i, token_id].item())\n",
        "            individual_token_probs.append(\n",
        "                (self.processor.decode(token_id), round(token_prob, round_to))\n",
        "            )\n",
        "        average_token_propabiility = round(\n",
        "            sum(token_prob for _, token_prob in individual_token_probs)\n",
        "            / len(individual_token_probs),\n",
        "            round_to,\n",
        "        )\n",
        "        return (\n",
        "            response,\n",
        "            total_probability_for_gen,\n",
        "            average_token_propabiility,\n",
        "            individual_token_probs,\n",
        "            generate_output,\n",
        "        )\n",
        "\n",
        "    def test_on_triviaQA(self, filename=\"triviaQA.ndjson\", n=100):\n",
        "        \"\"\"\n",
        "        Method to test hallucination detection methods on the\n",
        "        TriviaQA dataset. Streams results into an ndjson file.\n",
        "\n",
        "        Args:\n",
        "            filename: str: The filename to save the results to.\n",
        "            n: int: The number of samples to test on.\n",
        "        \"\"\"\n",
        "        logging.info(\n",
        "            \"LLMAgent testing hallucination detection on TriviaQA dataset with %s samples.\",\n",
        "            n,\n",
        "        )\n",
        "        dataset = load_dataset(\"trivia_qa\", \"rc\", split=\"train\", streaming=True)\n",
        "        iterator = iter(dataset)\n",
        "        for i in range(n):\n",
        "            logging.info(\"LLMAgent processing TriviaQA sample %s of %s.\", i + 1, n)\n",
        "            entry = next(iterator)\n",
        "            question = entry[\"question\"]\n",
        "            answer = entry[\"answer\"][\"value\"]\n",
        "            (\n",
        "                response,\n",
        "                total_probability,\n",
        "                average_token_probability,\n",
        "                _,\n",
        "                generate_output,\n",
        "            ) = self.generate_with_probability(question, max_tokens=15)\n",
        "            is_same, max_similarity = self.check_answer(response, [answer])\n",
        "            target_token = self.processor.encode(self.assistant_prompt_start)[0]\n",
        "            for i, token_id in enumerate(generate_output.sequences[0]):\n",
        "                if token_id == target_token:\n",
        "                    target_token_index = i - 1\n",
        "                    break\n",
        "            row = {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"response\": response,\n",
        "                \"correct\": is_same,\n",
        "                \"similarity\": max_similarity,\n",
        "                \"total_probability\": total_probability,\n",
        "                \"total_probability_predicts\": (\n",
        "                    True if total_probability > 0.5 else False\n",
        "                ),\n",
        "                \"average_token_probability\": average_token_probability,\n",
        "                \"average_token_propability_predicts\": (\n",
        "                    True if average_token_probability > 0.75 else False\n",
        "                ),\n",
        "                \"both_metrics_predict\": (\n",
        "                    True\n",
        "                    if (total_probability + average_token_probability) / 2 > 0.625\n",
        "                    else False\n",
        "                ),\n",
        "                \"middle_layer_activations_prompt\": self.visualise_activation_map_at_layer_at_token(\n",
        "                    generate_output, 16, target_token_index, return_numeric_state=True\n",
        "                ).tolist(),\n",
        "                \"middle_layer_activations_response\": self.visualise_activation_map_at_layer_at_token(\n",
        "                    generate_output, 16, -1, return_numeric_state=True\n",
        "                ).tolist(),\n",
        "                \"final_layer_activations_prompt\": self.visualise_activation_map_at_layer_at_token(\n",
        "                    generate_output, -1, target_token_index, return_numeric_state=True\n",
        "                ).tolist(),\n",
        "                \"final_layer_activations_response\": self.visualise_activation_map_at_layer_at_token(\n",
        "                    generate_output, -1, -1, return_numeric_state=True\n",
        "                ).tolist(),\n",
        "            }\n",
        "            with open(filename, \"a\") as file:\n",
        "                file.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "    def check_answer(self, query, comparisons):\n",
        "        \"\"\"\n",
        "        Method to check is an output answer is close\n",
        "        enough to any provided comparison answers.\n",
        "\n",
        "        Args:\n",
        "            query: str: The first sentence.\n",
        "            comparisons: list[str]: Comparison sentences.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the sentence matches any of the comparisons.\n",
        "            float: The max similarity score.\n",
        "        \"\"\"\n",
        "        if not query.startswith(self.embedding_instruction):\n",
        "            query = self.embedding_instruction + query\n",
        "        inputs = [query] + comparisons\n",
        "        embeddings = self.embedding_model.encode(inputs)\n",
        "        similarities = cos_sim(embeddings[0], embeddings[1:])[0].tolist()\n",
        "        is_same = False\n",
        "        for similarity in similarities:\n",
        "            if similarity > self.embedding_sameness_threshold:\n",
        "                return True, max(similarities)\n",
        "        return False, max(similarities)\n",
        "\n",
        "    def visualise_and_stack_layers(self, outputs, alpha=0.15, gap=100):\n",
        "        \"\"\"\n",
        "        Method to visualise the activations of all layers and\n",
        "        stack them into a single image.\n",
        "\n",
        "        Args:\n",
        "            outputs: dict: The outputs of the model.\n",
        "            alpha: float: The alpha value for the image.\n",
        "            gap: int: The gap between layers.\n",
        "\n",
        "        Returns:\n",
        "            bytes: The image bytes.\n",
        "        \"\"\"\n",
        "        logging.info(\"LLMAgent visualising and stacking activations for sequence.\")\n",
        "        num_layers = len(outputs.hidden_states[0])\n",
        "        logging.info(\"LLMAgent visualising activations for %s layers.\", num_layers)\n",
        "        layer_activation_images_bytes = []\n",
        "        for i in range(num_layers):\n",
        "            image_bytes = self.visualise_layer_activations(outputs, layer=i)\n",
        "            layer_activation_images_bytes.append(image_bytes)\n",
        "        logging.info(\"LLMAgent stacking activation images.\")\n",
        "        images = np.array(\n",
        "            [\n",
        "                Image.open(io.BytesIO(image_bytes)).resize((100, 100), Image.LANCZOS)\n",
        "                for image_bytes in layer_activation_images_bytes\n",
        "            ]\n",
        "        )\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        ax = fig.add_subplot(111, projection=\"3d\")\n",
        "        fig.patch.set_alpha(0)\n",
        "        ax.patch.set_alpha(0)\n",
        "        num_images, height, width, _ = images.shape\n",
        "\n",
        "        for i in range(num_images):\n",
        "            img = images[i]\n",
        "            x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
        "            z = np.full_like(x, i * gap)\n",
        "\n",
        "            img_normalized = img / 255.0\n",
        "            facecolors = np.empty(img_normalized.shape, dtype=img_normalized.dtype)\n",
        "            facecolors[..., :3] = img_normalized[..., :3]\n",
        "            facecolors[..., 3] = img_normalized[..., 3] * alpha\n",
        "\n",
        "            ax.plot_surface(\n",
        "                x, y, z, rstride=1, cstride=1, facecolors=facecolors, shade=False\n",
        "            )\n",
        "\n",
        "        ax.set_xlim(0, width)\n",
        "        ax.set_ylim(0, height)\n",
        "        ax.set_zlim(0, num_images * gap)\n",
        "        ax.view_init(elev=30, azim=30)\n",
        "        ax.grid(False)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_zticks([])\n",
        "        plt.tight_layout()\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format=\"png\")\n",
        "        buffer.seek(0)\n",
        "        image_bytes = buffer.read()\n",
        "        plt.close()\n",
        "\n",
        "        return image_bytes\n",
        "\n",
        "    def visualise_layer_activations(self, outputs, layer=0):\n",
        "        \"\"\"\n",
        "        Method to visualise the per-neuron activations for a given layer.\n",
        "\n",
        "        Args:\n",
        "            outputs: dict: The outputs of the model.\n",
        "            layer: int: The layer to visualise.\n",
        "\n",
        "        Returns:\n",
        "            bytes: The image bytes.\n",
        "        \"\"\"\n",
        "        logging.info(\n",
        "            \"LLMAgent visualising activations for layer %s for sequence.\", layer\n",
        "        )\n",
        "        tokens = [\n",
        "            self.processor.decode(input_token) for input_token in outputs.sequences[0]\n",
        "        ]\n",
        "\n",
        "        layer_feature_maps = []\n",
        "        for tensor in outputs.hidden_states:\n",
        "            target_layer = tensor[layer]\n",
        "            tokens_in_tensor = target_layer.shape[1]\n",
        "            for i in range(tokens_in_tensor):\n",
        "                feature_map = target_layer[0, i, :].cpu().detach().numpy()\n",
        "                layer_feature_maps.append(feature_map)\n",
        "\n",
        "        total_tokens = len(layer_feature_maps)\n",
        "        grid_size = int(np.ceil(np.sqrt(total_tokens)))\n",
        "        plt.figure(figsize=(100, 100))\n",
        "        plt.gca().patch.set_alpha(0)\n",
        "\n",
        "        for idx, feature_map in enumerate(layer_feature_maps):\n",
        "            n_activations = len(feature_map)\n",
        "            heatmap_size = int(np.ceil(np.sqrt(n_activations)))\n",
        "            padded_activations = np.pad(\n",
        "                feature_map, (0, heatmap_size**2 - n_activations), mode=\"constant\"\n",
        "            )\n",
        "            activation_grid = padded_activations.reshape(heatmap_size, heatmap_size)\n",
        "\n",
        "            ax = plt.subplot(grid_size, grid_size, idx + 1)\n",
        "            sns.heatmap(\n",
        "                activation_grid,\n",
        "                cmap=\"mako_r\",\n",
        "                cbar=False,\n",
        "                linecolor=\"lightgrey\",\n",
        "                linewidths=0.2,\n",
        "                xticklabels=False,\n",
        "                yticklabels=False,\n",
        "            )\n",
        "            ax.text(\n",
        "                0.5,\n",
        "                0.5,\n",
        "                tokens[idx],\n",
        "                fontsize=80,\n",
        "                color=\"white\",\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                alpha=0.6,\n",
        "                transform=ax.transAxes,\n",
        "                weight=\"bold\",\n",
        "            )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format=\"png\", transparent=True)\n",
        "        buffer.seek(0)\n",
        "        image_bytes = buffer.read()\n",
        "        plt.close()\n",
        "        logging.info(\"LLMAgent visualised activations for layer %s.\", layer)\n",
        "        return image_bytes\n",
        "\n",
        "    def visualise_activation_map_at_layer_at_token(\n",
        "        self, outputs, layer, token, return_numeric_state=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Method to visualise the activation map at a given layer\n",
        "        and token.\n",
        "\n",
        "        Args:\n",
        "            outputs: dict: The outputs of the model.\n",
        "            layer: int: The layer to visualise.\n",
        "            token: int: The token to visualise.\n",
        "            return_numeric_state: bool: Whether to return the raw state instead of image bytes.\n",
        "\n",
        "        Returns:\n",
        "            bytes: The image bytes. | numpy.ndarray: The numeric activation state.\n",
        "        \"\"\"\n",
        "        logging.info(\n",
        "            \"LLMAgent visualising activations for layer %s and token %s.\",\n",
        "            layer,\n",
        "            token,\n",
        "        )\n",
        "        tokens = [\n",
        "            self.processor.decode(input_token) for input_token in outputs.sequences[0]\n",
        "        ]\n",
        "\n",
        "        layer_feature_maps = []\n",
        "        for tensor in outputs.hidden_states:\n",
        "            target_layer = tensor[layer]\n",
        "            tokens_in_tensor = target_layer.shape[1]\n",
        "            for i in range(tokens_in_tensor):\n",
        "                feature_map = target_layer[0, i, :].cpu().detach().numpy()\n",
        "                layer_feature_maps.append(feature_map)\n",
        "\n",
        "        token_feature_map = layer_feature_maps[token]\n",
        "        if return_numeric_state:\n",
        "            return token_feature_map\n",
        "\n",
        "        total_tokens = len(layer_feature_maps)\n",
        "        grid_size = int(np.ceil(np.sqrt(total_tokens)))\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.gca().patch.set_alpha(0)\n",
        "\n",
        "        n_activations = len(token_feature_map)\n",
        "        heatmap_size = int(np.ceil(np.sqrt(n_activations)))\n",
        "        padded_activations = np.pad(\n",
        "            token_feature_map, (0, heatmap_size**2 - n_activations), mode=\"constant\"\n",
        "        )\n",
        "        activation_grid = padded_activations.reshape(heatmap_size, heatmap_size)\n",
        "        ax = plt.subplot(1, 1, 1)\n",
        "        sns.heatmap(\n",
        "            activation_grid,\n",
        "            cmap=\"mako_r\",\n",
        "            cbar=False,\n",
        "            linecolor=\"lightgrey\",\n",
        "            linewidths=0.2,\n",
        "            xticklabels=False,\n",
        "            yticklabels=False,\n",
        "        )\n",
        "        ax.text(\n",
        "            0.5,\n",
        "            0.5,\n",
        "            tokens[token],\n",
        "            fontsize=80,\n",
        "            color=\"white\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            alpha=0.6,\n",
        "            transform=ax.transAxes,\n",
        "            weight=\"bold\",\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format=\"png\", transparent=True)\n",
        "        buffer.seek(0)\n",
        "        image_bytes = buffer.read()\n",
        "        plt.close()\n",
        "        logging.info(\n",
        "            \"LLMAgent visualised activations for layer %s and token %s.\", layer, token\n",
        "        )\n",
        "        return image_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYNxX-wWiHiW"
      },
      "source": [
        "# Instantiate object and load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d8iKgHQbh2kR"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from IPython.display import display\n",
        "\n",
        "agent = None\n",
        "agent = LLMAgent(\n",
        "    system_prompt=\"You are a helpful AI assistant that provides concise answers.\",\n",
        "    use_gpu=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MxP4BTFfUEV"
      },
      "source": [
        "# Generate 3d model state cube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oXkz-Bx2ffVi"
      },
      "outputs": [],
      "source": [
        "response_dict = agent.generate_with_response_dict(\"What is the capital of Australia?\")\n",
        "\n",
        "image_bytes = agent.visualise_and_stack_layers(response_dict)\n",
        "\n",
        "image = Image.open(io.BytesIO(image_bytes))\n",
        "clear_output()\n",
        "display(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFFa25nSiWET"
      },
      "source": [
        "# Generate 2d layer state for sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHh9EvNKicGa"
      },
      "outputs": [],
      "source": [
        "response_dict = agent.generate_with_response_dict(\"What is the capital of Australia?\")\n",
        "\n",
        "image_bytes = agent.visualise_layer_activations(response_dict, layer=16)\n",
        "\n",
        "image = Image.open(io.BytesIO(image_bytes))\n",
        "clear_output()\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfnUuqbDjDjX"
      },
      "source": [
        "# Generate 2d layer state for single token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTyM2ASJjHxF"
      },
      "outputs": [],
      "source": [
        "response_dict = agent.generate_with_response_dict(\"What is the capital of Australia?\")\n",
        "\n",
        "image_bytes = agent.visualise_activation_map_at_layer_at_token(response_dict, 16, 15)\n",
        "\n",
        "image = Image.open(io.BytesIO(image_bytes))\n",
        "clear_output()\n",
        "display(image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
